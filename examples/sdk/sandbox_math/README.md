# Sandbox Math Agent — Sandboxed Execution with Tinker

This example trains a math agent that runs inside a **sandboxed environment**
(local subprocess or Docker container) rather than inside the trainer process.
It is the sandbox counterpart of `adk_math`.

The agent code lives in a standalone `agent/` directory with its own
dependencies. During training the orchestrator uploads the agent code into
sandboxes, dispatches tasks over HTTP, and collects results through a
SQLite-backed result store. The agent only needs a standard OpenAI client —
no framework-specific imports required.

## Architecture

```
Host (trainer process)                          Sandbox (subprocess / container)
┌────────────────────────────────┐              ┌──────────────────────────────┐
│ AgentTrainer (Tinker backend)  │              │ worker_server.py             │
│  ├─ SdkWorkflowFactory        │              │  ├─ /health                  │
│  │   ├─ LiteLLM proxy         │◄── results ──│  ├─ /execute (fire & forget) │
│  │   │   ├─ TracingCallback    │              │  │   ├─ generate session_uid │
│  │   │   └─ ResultStore routes │              │  │   ├─ encode into proxy URL│
│  │   ├─ ExecutionResultStore   │              │  │   ├─ call rollout(task,   │
│  │   └─ SandboxOrchestrator   │── tasks ────►│  │   │       config)         │
│  │       └─ worker pool / sem  │              │  │   └─ POST result to proxy │
│  └─ UnifiedWorkflowEngine     │              │  └─ agent/                   │
│      └─ SdkWorkflow (per task) │              │      └─ agent.py            │
│          └─ orchestrator.exec()│              │          └─ rollout()        │
│                                │              │              └─ OpenAI client│
│ Tinker RL training loop        │              │                  ▲           │
└────────────────────────────────┘              └──────────────────│───────────┘
                                                                  │
                                LLM calls (proxied base_url) ─────┘
```

1. The trainer starts the LiteLLM proxy with `--enable-result-store`.
2. `SandboxOrchestrator` creates sandbox workers and uploads `agent/` + the
   built-in `worker_server.py`.
3. For each task, the orchestrator POSTs to a worker's `/execute` endpoint.
4. The worker generates a `session_uid`, encodes it into the proxy URL via
   a metadata slug, and calls `agent.rollout(task, config)`.
5. The agent makes standard OpenAI-compatible LLM calls through the proxied
   URL. The proxy's `TracingCallback` captures token IDs and logprobs.
6. The worker serialises the returned `list[Trajectory]` and POSTs the result
   back to the proxy's `/rllm/results/{execution_id}` route.
7. The trainer polls the `ExecutionResultStore`, deserialises trajectories,
   merges them with proxy traces, and builds training-ready Episodes.

## Files

| File | Description |
|------|-------------|
| `train.py` | Training script (host-side). Configures sandbox, launches trainer. Includes `--test-sandbox` smoke test. |
| `agent/agent.py` | Standalone agent: `rollout(task, config) -> list[Trajectory]`. Uses the OpenAI client against the proxied URL. |
| `agent/requirements.txt` | Agent-side dependencies (just `openai`). |
| `Dockerfile.agentcore` | Reference Dockerfile for AgentCore (normally auto-generated by `agentcore configure`). |
| `requirements-agentcore.txt` | Container-side dependencies for AgentCore deployment (`bedrock-agentcore`, `openai`). |

## Agent Contract

The agent module must expose a function with this signature:

```python
def rollout(task: dict, config: dict) -> list[Trajectory]:
    ...
```

The `config` dict always contains:

| Key | Description |
|-----|-------------|
| `base_url` | Proxied OpenAI-compatible endpoint with session metadata encoded in the URL |
| `session_uid` | Unique ID for trace correlation |
| `model_id` | Model name to use for completions |

The agent creates an OpenAI client pointing at `base_url`, runs its logic,
computes a reward, and returns one or more `Trajectory` objects.

## Quick Start

### 1. Prepare dataset

```bash
python -m examples.countdown.prepare_countdown_data
```

### 2. Smoke test (no GPU, no model)

Validates the full sandbox plumbing — sandbox creation, agent upload, task
dispatch, LLM call (mocked), result push, and result retrieval:

```bash
python -m examples.sdk.sandbox_math.train --test-sandbox
```

Expected output:

```
Agent directory: .../examples/sdk/sandbox_math/agent
Test server listening on http://127.0.0.1:18999/v1
Creating local sandbox orchestrator...
Dispatching test task...
Result: success=True, session_uid=...
  trajectories: 1 returned
  traj[0]: name=solver, reward=1.0, steps=1

Sandbox smoke test PASSED (reward=1.0)
```

### 3. Run training

```bash
python -m examples.sdk.sandbox_math.train \
    rllm/backend=tinker \
    model.name=Qwen/Qwen3-8B \
    training.group_size=8 \
    data.train_batch_size=4 \
    rllm.trainer.test_freq=5 \
    rllm.trainer.val_before_train=false
```

## Configuration

Sandbox settings live under `rllm.sdk.sandbox` and can be overridden on the
command line:

| Setting | Default | Description |
|---------|---------|-------------|
| `enabled` | `false` | Enable sandboxed execution |
| `backend` | `local` | Sandbox backend: `local`, `docker`, `modal`, `agentcore` |
| `agent_dir` | `""` | Path to the agent project directory |
| `agent_module` | `agent` | Python module name containing the rollout function |
| `agent_func` | `rollout` | Function name in the agent module |
| `pool_mode` | `persistent` | `persistent` (warm pool) or `per_task` (fresh sandbox per task) |
| `num_workers` | `8` | Number of persistent workers |
| `worker_port` | `8100` | Base port for worker servers |
| `image` | `python:3.11-slim` | Docker image (when `backend=docker`) |
| `install_rllm_sdk` | `true` | Install `rllm[sdk]` in the sandbox |
| `execution_timeout` | `600.0` | Max seconds to wait for a single task |

### Example: Docker backend

```bash
python -m examples.sdk.sandbox_math.train \
    rllm/backend=tinker \
    model.name=Qwen/Qwen3-8B \
    rllm.sdk.sandbox.backend=docker \
    rllm.sdk.sandbox.image=python:3.11-slim \
    rllm.sdk.sandbox.num_workers=16
```

### Example: Per-task sandboxes

```bash
python -m examples.sdk.sandbox_math.train \
    rllm/backend=tinker \
    model.name=Qwen/Qwen3-8B \
    rllm.sdk.sandbox.pool_mode=per_task \
    rllm.sdk.sandbox.max_concurrent=32
```

### Example: AWS Bedrock AgentCore backend

The `agentcore` backend invokes a pre-deployed AgentCore Runtime via `boto3`
instead of managing local sandboxes. The ACR container runs
`agentcore_worker.py` and POSTs results directly to the proxy — same
`ExecutionResultStore` + SQLite polling as local/docker, no S3 or extra SDK.

#### Step 1 — Install dependencies

On the trainer host:

```bash
pip install boto3
```

In the example directory (for the `agentcore` CLI):

```bash
pip install bedrock-agentcore-starter-toolkit
```

#### Step 2 — Prepare the agent directory

Copy the self-contained ACR worker into the example directory (it has no
rllm dependency and is designed to run standalone in the container):

```bash
cd examples/sdk/sandbox_math
cp ../../../rllm/sdk/sandbox/agentcore_worker.py .
```

#### Step 3 — Configure the AgentCore Runtime

The `agentcore` CLI generates a Dockerfile and deployment config
automatically. For a public-facing proxy:

```bash
agentcore configure \
    --entrypoint agentcore_worker.py \
    --name rllm_sandbox_math \
    --requirements-file requirements-agentcore.txt \
    --deployment-type container \
    --disable-memory \
    --non-interactive
```

For a proxy inside a VPC, add network config:

```bash
agentcore configure \
    --entrypoint agentcore_worker.py \
    --name rllm_sandbox_math \
    --requirements-file requirements-agentcore.txt \
    --deployment-type container \
    --vpc \
    --subnets subnet-xxxx \
    --security-groups sg-xxxx \
    --disable-memory \
    --non-interactive
```

This creates `.bedrock_agentcore/rllm_sandbox_math/Dockerfile` and a
`.bedrock_agentcore.yaml` config file.

The generated Dockerfile runs `python -m agentcore_worker` without CLI
args. The worker reads its config from environment variables instead.
Add these to the generated Dockerfile (before the `CMD` line):

```dockerfile
ENV RLLM_AGENT_MODULE=agent \
    RLLM_AGENT_FUNC=rollout \
    RLLM_AGENT_DIR=/app/agent
```

#### Step 4 — Test locally (optional)

Build and run the container to verify it starts correctly:

```bash
docker build -t rllm-sandbox-math:dev \
    -f .bedrock_agentcore/rllm_sandbox_math/Dockerfile .

docker run -p 8080:8080 rllm-sandbox-math:dev

# Health check
curl http://localhost:8080/ping
```

#### Step 5 — Deploy to AgentCore

This builds an ARM64 container via CodeBuild, pushes it to ECR, and
creates/updates the AgentCore Runtime — all in one command:

```bash
agentcore deploy --agent rllm_sandbox_math
```

The command prints the runtime ARN when complete. You can also retrieve it
later:

```bash
agentcore list
```

#### Step 6 — Grant IAM permissions

Your IAM user/role needs permission to invoke the AgentCore Runtime.
Attach an inline policy (replace the ARN with your runtime ARN from
Step 5):

```bash
aws iam put-user-policy \
    --user-name <YOUR_IAM_USER> \
    --policy-name BedrockAgentCoreInvoke \
    --policy-document '{
        "Version": "2012-10-17",
        "Statement": [{
            "Effect": "Allow",
            "Action": "bedrock-agentcore:InvokeAgentRuntime",
            "Resource": "arn:aws:bedrock-agentcore:us-west-2:<ACCOUNT_ID>:runtime/*"
        }]
    }'
```

Or scope the `Resource` to your specific runtime ARN for tighter
permissions.

#### Step 7 — Run training

The ACR container must reach your LiteLLM proxy for both LLM calls and
result POSTs. Since the proxy defaults to `127.0.0.1:4000`, you **must**
set `base_url` to a reachable address and bind the proxy to `0.0.0.0`:

```bash
python -m examples.sdk.sandbox_math.train \
    rllm/backend=tinker \
    model.name=Qwen/Qwen3-8B \
    training.group_size=8 \
    data.train_batch_size=4 \
    rllm.trainer.val_before_train=false
    rllm.sdk.proxy.host=0.0.0.0 \
    rllm.sdk.sandbox.backend=agentcore \
    'rllm.sdk.sandbox.extra={agent_runtime_arn: "arn:aws:bedrock-agentcore:us-west-2:123456789012:runtime/rllm_sandbox_math_abc123", base_url: "http://<YOUR_HOST_IP>:4000/v1"}'
```

Replace `<YOUR_HOST_IP>` with the IP address reachable from the ACR
container:
- **Public network mode**: Use the host's public IP or DNS name.
- **VPC mode**: Use the host's private IP (ensure the security group
  allows inbound on port 4000 from the ACR subnet).

> **Tip**: Find your IP with `curl -s ifconfig.me` (public) or
> `hostname -I | awk '{print $1}'` (private).

#### AgentCore `extra` fields

| Key | Required | Default | Description |
|-----|----------|---------|-------------|
| `agent_runtime_arn` | Yes | — | ARN of the AgentCore Runtime (from Step 5) |
| `base_url` | Yes* | proxy localhost URL | Reachable proxy URL for the ACR container (e.g. `http://<host-ip>:4000/v1`). *Required unless the proxy is publicly reachable at its default address. |
| `tps_limit` | No | `25` | Max concurrent invocations (asyncio.Semaphore) |
| `max_retry_attempts` | No | `5` | boto3 adaptive retry max attempts |

## How It Differs from Other Examples

| Aspect | `sandbox_math` (this) | `adk_math` | `strands_math` |
|--------|-----------------------|------------|----------------|
| Where agent runs | Sandbox (subprocess / Docker) | Trainer process | Trainer process |
| Agent framework | Plain OpenAI client | Google ADK | Strands Agents SDK |
| Agent provided as | Standalone directory (`agent/`) | `agent_run_func` callable | `agent_run_func` callable |
| Dependency isolation | Full (separate environment) | Shared with trainer | Shared with trainer |
| Result delivery | Fire-and-forget push to SQLite | Direct return value | Direct return value |
| Config entry point | `rllm.sdk.sandbox.*` | `agent_run_func=` kwarg | `agent_run_func=` kwarg |
