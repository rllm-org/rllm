"""vLLM instrumentation for token IDs support (vLLM < 0.10.2).

This module provides monkey patching to add token IDs to vLLM's
OpenAI-compatible API responses for versions before 0.10.2, matching the
native vLLM 0.10.2+ format.

Native vLLM 0.10.2+ format:
- prompt_token_ids: list[int] at top level (for the prompt)
- token_ids: list[int] per choice (for response tokens)

For vLLM >= 0.10.2, use the native `return_token_ids=True` parameter instead.

Example:
    >>> from rllm.engine.vllm_instrumentation import instrument_vllm
    >>> instrument_vllm()
    >>> # Now all vLLM responses include token IDs in native format
"""

from __future__ import annotations

import logging
from typing import Any

from pydantic import Field

logger = logging.getLogger(__name__)

__all__ = [
    "instrument_vllm",
    "is_vllm_instrumented",
]


# Store original references and state
_original_chat_completion_full_generator = None
_original_response_class = None
_is_instrumented = False


def _check_vllm_available() -> bool:
    """Check if vLLM is available."""
    try:
        import vllm  # noqa: F401

        return True
    except ImportError:
        return False


def _get_vllm_version() -> tuple[int, int, int] | None:
    """Get vLLM version as tuple."""
    try:
        import vllm

        version_str = vllm.__version__
        # Parse version like "0.9.1" or "0.10.2"
        parts = version_str.split(".")[:3]
        return tuple(int(p) for p in parts)
    except Exception:
        return None


def instrument_vllm(force: bool = False, add_response_logprobs: bool = False) -> bool:
    """Instrument vLLM to capture token IDs and logprobs generated by engine.

    This instrumentation adds token ID fields to vLLM's ChatCompletionResponse
    for versions < 0.10.2, matching the native vLLM 0.10.2+ format:
    - prompt_token_ids at top level (for the prompt)
    - token_ids per choice (for response tokens)
    - response_logprobs per choice (compact float list, if enabled)

    For vLLM >= 0.10.2, this instrumentation is not needed as the feature is
    natively supported via the `return_token_ids=True` parameter.

    Args:
        force: If True, apply instrumentation even if vLLM >= 0.10.2.
               Default is False (auto-detect version).
        add_response_logprobs: If True, extract per-token logprobs as a compact
                               float list in addition to token IDs. This avoids
                               the verbose OpenAI logprobs format.
                               Default is False.

    Returns:
        True if instrumentation was applied, False otherwise.

    Example:
        >>> from rllm.engine.vllm_instrumentation import instrument_vllm
        >>> instrument_vllm(add_response_logprobs=True)
        >>> # Now all vLLM responses include token IDs and compact logprobs
    """
    # Force import vLLM modules to ensure they're loaded before patching
    try:
        logger.info("[VLLM_INSTRUMENT] Force importing vLLM modules...")
        import vllm
        import vllm.entrypoints.openai.protocol
        import vllm.entrypoints.openai.serving_chat

        logger.info(f"[VLLM_INSTRUMENT] vLLM modules imported successfully, vllm.__version__={vllm.__version__}")
    except ImportError as e:
        logger.warning(f"âš ï¸ [VLLM_INSTRUMENT] vLLM not available: {e}")
        return False

    if not _check_vllm_available():
        logger.warning("âš ï¸ [VLLM_INSTRUMENT] vLLM not available, skipping instrumentation")
        return False

    # Check version
    version = _get_vllm_version()
    logger.info(f"ðŸ” [VLLM_INSTRUMENT] vLLM version detected: {version}")
    if version and version >= (0, 10, 2) and not force:
        msg = f"vLLM {'.'.join(map(str, version))} has native return_token_ids support. Instrumentation not needed."
        logger.info(f"â„¹ï¸ [VLLM_INSTRUMENT] {msg}")
        return False

    try:
        import vllm.entrypoints.openai.protocol
        from vllm.entrypoints.openai.protocol import ChatCompletionResponse
        from vllm.entrypoints.openai.serving_chat import OpenAIServingChat

        # Check if already instrumented
        global _is_instrumented, _original_chat_completion_full_generator, _original_response_class

        if _is_instrumented:
            logger.warning("âš ï¸ [VLLM_INSTRUMENT] vLLM is already instrumented by RLLM. Skipping.")
            return False

        logger.info("ðŸ”§ [VLLM_INSTRUMENT] Starting vLLM instrumentation...")

        # Store original references
        _original_chat_completion_full_generator = OpenAIServingChat.chat_completion_full_generator
        _original_response_class = ChatCompletionResponse

        logger.info(f"ðŸ“¦ [VLLM_INSTRUMENT] Stored original chat_completion_full_generator: {_original_chat_completion_full_generator}")
        logger.info(f"ðŸ“¦ [VLLM_INSTRUMENT] Stored original ChatCompletionResponse: {_original_response_class}")

        # Import ChatCompletionResponseChoice to patch it
        from vllm.entrypoints.openai.protocol import ChatCompletionResponseChoice

        # Create patched choice class with token_ids and response_logprobs per choice
        # (matching native vLLM 0.10.2+ format for token_ids, plus compact logprobs)
        class ChatCompletionResponseChoicePatched(ChatCompletionResponseChoice):
            """Extended ChatCompletionResponseChoice with token IDs and compact logprobs."""

            # Native vLLM 0.10.2+ has token_ids per choice (for response tokens)
            # These fields are response-only and should not appear in requests
            token_ids: list[int] | None = Field(default=None, exclude=False)
            # Compact logprobs list (RLLM extension)
            response_logprobs: list[float] | None = Field(default=None, exclude=False)

        # Create patched response class with prompt_token_ids at top level
        # (matching native vLLM 0.10.2+ format)
        class ChatCompletionResponsePatched(ChatCompletionResponse):
            """Extended ChatCompletionResponse with token IDs matching native vLLM 0.10.2+ format."""

            # Native vLLM 0.10.2+ has prompt_token_ids at top level
            # This field is response-only and should not appear in requests
            prompt_token_ids: list[int] | None = Field(default=None, exclude=False)
            choices: list[ChatCompletionResponseChoicePatched]

        # Create patched generator
        async def chat_completion_full_generator(
            self: Any,
            request: Any,
            result_generator: Any,
            request_id: str,
            model_name: str,
            conversation: Any,
            tokenizer: Any,
            request_metadata: Any,
        ) -> Any:
            """Patched generator that extracts token IDs and logprobs in native vLLM 0.10.2+ format."""
            logger.debug(f"ðŸŽ¯ [VLLM_INSTRUMENT] Patched generator called for request_id={request_id}")
            prompt_token_ids: list[int] | None = None
            response_token_ids: list[list[int]] | None = None
            response_logprobs_lists: list[list[float]] | None = None

            async def _generate_interceptor():
                """Intercept generator to extract token IDs and logprobs."""
                nonlocal prompt_token_ids, response_token_ids, response_logprobs_lists
                async for res in result_generator:
                    yield res

                    # Extract token IDs from vLLM's internal RequestOutput
                    prompt_token_ids = res.prompt_token_ids
                    response_token_ids = [output.token_ids for output in res.outputs]

                    # Extract logprobs if requested
                    if add_response_logprobs:
                        response_logprobs_lists = []
                        for output in res.outputs:
                            if output.logprobs:  # list[LogprobsOnePosition]: list of dict[int, Logprob]
                                curr_log_probs = [output.logprobs[i][token_id].logprob if i < len(output.logprobs) and token_id in output.logprobs[i] else float("-inf") for i, token_id in enumerate(output.token_ids)]
                                response_logprobs_lists.append(curr_log_probs)
                            else:
                                response_logprobs_lists.append([])

            # Call original generator with interceptor
            response = await _original_chat_completion_full_generator(
                self,
                request,
                _generate_interceptor(),
                request_id,
                model_name,
                conversation,
                tokenizer,
                request_metadata,
            )

            # Add token IDs and logprobs to response in native vLLM 0.10.2+ format:
            # - prompt_token_ids at top level (for the prompt)
            # - token_ids per choice (for response tokens)
            # - response_logprobs per choice (compact float list, if enabled)

            # Update choices to add response token_ids and response_logprobs
            updated_choices = []
            for i, choice in enumerate(response.choices):
                # Get response token IDs for this choice
                choice_token_ids = response_token_ids[i] if response_token_ids and i < len(response_token_ids) else []

                # Convert choice to dict and update with new fields
                choice_dict = choice.model_dump()
                choice_dict["token_ids"] = choice_token_ids

                if add_response_logprobs:
                    choice_dict["response_logprobs"] = response_logprobs_lists[i] if i < len(response_logprobs_lists) else []
                    choice_dict["logprobs"] = None

                # Create new ChatCompletionResponseChoicePatched instance to ensure field is in schema
                updated_choice = ChatCompletionResponseChoicePatched(**choice_dict)
                updated_choices.append(updated_choice)

            # Update response with prompt_token_ids at top level and updated choices
            # Convert to ChatCompletionResponsePatched to ensure fields are in schema
            response_dict = response.model_dump()
            response_dict["prompt_token_ids"] = prompt_token_ids
            response_dict["choices"] = updated_choices
            response = ChatCompletionResponsePatched(**response_dict)

            return response

        # Apply patches
        logger.info("ðŸ”¨ [VLLM_INSTRUMENT] Applying patches to vLLM...")
        vllm.entrypoints.openai.protocol.ChatCompletionResponse = ChatCompletionResponsePatched
        OpenAIServingChat.chat_completion_full_generator = chat_completion_full_generator
        _is_instrumented = True

        logger.info("âœ… [VLLM_INSTRUMENT] vLLM instrumented successfully to return token IDs")
        logger.info(f"ðŸ“Š [VLLM_INSTRUMENT] Patched ChatCompletionResponse: {ChatCompletionResponsePatched}")

        logger.info(f"ðŸ“Š [VLLM_INSTRUMENT] Patched generator: {chat_completion_full_generator}")
        return True

    except Exception as e:
        logger.error(f"âŒ [VLLM_INSTRUMENT] Failed to instrument vLLM: {e}")
        import traceback

        traceback.print_exc()
        return False


def is_vllm_instrumented() -> bool:
    """Check if vLLM is currently instrumented.

    Returns:
        True if vLLM is instrumented, False otherwise.
    """
    return _is_instrumented


def get_vllm_token_ids_support() -> str:
    """Get information about vLLM token IDs support.

    Returns:
        One of:
        - "native": vLLM >= 0.10.2 with native support
        - "instrumented": vLLM < 0.10.2 with RLLM instrumentation
        - "none": vLLM < 0.10.2 without instrumentation
        - "unavailable": vLLM not installed
    """
    if not _check_vllm_available():
        return "unavailable"

    version = _get_vllm_version()
    if version and version >= (0, 10, 2):
        return "native"

    if is_vllm_instrumented():
        return "instrumented"

    return "none"


def check_vllm_instrumentation_status() -> dict:
    """Check detailed vLLM instrumentation status in current process.

    This is useful for debugging whether the instrumentation is present in Ray workers.

    Returns:
        Dictionary with instrumentation status details
    """
    status = {
        "vllm_available": _check_vllm_available(),
        "vllm_version": None,
        "is_instrumented_flag": _is_instrumented,
        "has_patched_generator": False,
        "has_patched_response_class": False,
    }

    if not status["vllm_available"]:
        return status

    try:
        import vllm

        status["vllm_version"] = vllm.__version__

        from vllm.entrypoints.openai.protocol import ChatCompletionResponse
        from vllm.entrypoints.openai.serving_chat import OpenAIServingChat

        # Check if the generator is patched by looking at its name or checking if it's our function
        generator = OpenAIServingChat.chat_completion_full_generator
        if generator is not None:
            # Our patched function will have specific attributes or name
            status["generator_name"] = getattr(generator, "__name__", str(generator))
            status["has_patched_generator"] = "Patched generator that extracts token IDs" in str(generator.__doc__ or "")

        # Check if response class has our custom fields
        status["has_patched_response_class"] = hasattr(ChatCompletionResponse, "__annotations__") and "prompt_token_ids" in getattr(ChatCompletionResponse, "__annotations__", {})

    except Exception as e:
        status["error"] = str(e)

    logger.info(f"[VLLM_INSTRUMENT] Instrumentation status check: {status}")
    return status
