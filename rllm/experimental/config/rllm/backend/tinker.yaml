# @package _global_

# Tinker Backend Configuration for rLLM
# This config is used when training agents with Tinker backend
# Unified configuration supporting both workflow and agent training modes
# Default settings match tinker_cookbook.recipes.math_rl for MATH dataset

# Tinker-specific settings
tinker_base_url: null  # Tinker service URL (null for local)
fuse_forward_backward_and_optim_step: false
  
# Model Configuration
model:
  name: "Qwen/Qwen3-8B"  # Default model for MATH dataset
  lora_rank: 32
  train_unembed: true  # Train LoRA on output embedding layer (set to false for Fireworks compatibility)
  train_attn: true     # Train LoRA on attention layers
  train_mlp: true      # Train LoRA on MLP layers

# Training Configuration (tinker-specific)
training:
  group_size: ???  # Number of rollouts per prompt (for GRPO)
  learning_rate: 2e-5  # 2e-5 for MATH dataset
  lr_schedule: "constant"  # "constant", "linear", "cosine"
  warmup_steps_ratio: 0.0  # Warmup steps as ratio of total steps (between 0 and 1)
  # below are the Adam optimizer parameters
  beta1: 0.9
  beta2: 0.95
  eps: 1e-8
  max_length: 32768
  num_minibatches: 1
  default_local_dir: '/tmp/rllm-tinker-checkpoints'
  resume_from_tinker_id: null  # Tinker model ID path to resume from (e.g., tinker://uuid/weights/000060)

# Validation Configuration
validation:
  group_size: ???

# Sampling Configuration
sampling:
  train:
    temperature: 1.0
    top_p: 1.0
    top_k: -1
  val:
    temperature: 1.0
    top_p: 1.0
    top_k: -1

# Workflow Configuration (for workflow training mode)
workflow:
  n_parallel_tasks: 256
  retry_limit: 3 

# Agent Configuration (for agent training mode)
agent:
  max_steps: 1  # Single-turn vs multi-turn
  agent_args: {}

# Tinker Engine Configuration
rollout_engine:
  reasoning_effort: "medium"
  accumulate_reasoning: false
  disable_thinking: false
  bypass_render_with_parser: true
  renderer_name: null

# Data Configuration
data:
  train_files: null
  val_files: null
  max_prompt_length: 2048
  max_response_length: 2048
  train_batch_size: 64
  val_batch_size: 32

# On-policy Self-Distillation (OPSD) Configuration
opsd:
  kl_penalty_coef: 1.0
  kl_discount_factor: 0.0
  teacher_messages_key: "teacher_messages"
  teacher_policy_update_freq: -1 # -1 for always using the initial policy

# Overriding the rLLM-common config with Tinker-specific ones
# Only override if the algorithm values are not None, otherwise still use the rLLM-common config
rllm:
  backend: tinker
  rollout:
    n: ${oc.select:training.group_size, 8}
    n_val: ${oc.select:validation.group_size, 1}