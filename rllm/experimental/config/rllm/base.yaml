# Common, backend-agnostic configuration for rLLM
# Agent Configuration
agent:
  name: math_agent
  max_steps: 20
  trajectory_timeout: null
  overlong_filter: False
  agent_args: {}
  engine_args: {}

# Environment Configuration
env:
  name: custom
  env_args: {}

# Workflow Configuration
workflow:
  use_workflow: False
  name: single_turn_workflow
  workflow_args:
    agent_cls: null
    agent_args: {}
    env_cls: null
    env_args: {}
    timeout: 1e6
    gamma: 0.0 # no discounting
    reward_bonus_coeff: 0.0 # no reward shaping
  n_parallel_tasks: 256
  retry_limit: 3
  raise_on_error: true

# Rollout Configuration (note: this is just for logging purpose, the actual rollout configs are backend-specific)
rollout:
  n: 8
  n_val: 1  # number of validation samples per prompt

# Trainer Configuration
trainer:
  total_epochs: 10
  total_batches: -1
  logger: ['console']  # Options: 'console', 'wandb', 'tensorboard'
  project_name: 'rllm-training'
  experiment_name: 'default'
  test_freq: 5
  save_freq: 20
  val_before_train: true
  val_only: false

# Algorithm Configuration
algorithm:
  adv_estimator: grpo # [grpo, reinforce, gae]
  gamma: 1.0
  lam: 0.95
  norm_adv_by_std_in_grpo: true
  use_rllm: false
  # for tinker backend only (avaiable options: importance_sampling, ppo, cispo, dro, cross_entropy)
  loss_fn: null

# Stepwise advantage
stepwise_advantage:
  enable: False
  mode: broadcast # [broadcast, per_step]
  normalize_by_steps: False

# Disable thinking
disable_thinking: False

# Accumulate reasoning
accumulate_reasoning: False

# Mask truncated samples
mask_truncated_samples: False
filter_token_mismatch: True

# Compact filtering
compact_filtering:
  enable: False
  mask_max_prompt_length_exceeded: True
  mask_max_response_length_exceeded: True
  mask_env_done: False
  mask_max_turns_exceeded: True
  mask_timeout: True
  mask_unknown: False
  mask_error: True

# Rejection sampling
rejection_sample:
  enable: False
  multiplier: 1
  min_partial_solve_tasks: 1
  min_trajs_per_group: 2

# SDK Configuration
sdk:
  store:
    path: ~/.rllm/traces.db
  processing:
    groupby_key: null
    traj_name_key: null
  proxy:
    host: 127.0.0.1
    port: 4000
    mode: subprocess  # external | subprocess
    admin_token: my-shared-secret

# Remote Agent Configuration
# When enabled, episodes are collected from remote agent runtimes (e.g. Docker
# containers) instead of running workflows locally.  The trainer exposes an
# OpenAI-compatible inference API that remote agents call for model inference.
remote_agent:
  enabled: false
  # Mode: "http" (persistent connection) or "agentcore" (fire-and-forget via ACR + S3/SQS)
  mode: "http"
  # List of remote agent endpoint URLs (e.g. ["http://agent1:8000", "http://agent2:8000"])
  endpoints: []
  # Inference API server settings (exposed by the trainer for remote agents)
  inference_api:
    host: "0.0.0.0"
    port: 8089
  # Per-episode HTTP timeout in seconds (used in "http" mode)
  timeout: 600
  # Maximum number of concurrent requests to remote agents
  max_concurrent: 128
  # Number of retry attempts for failed remote calls
  retry_limit: 3
  # AgentCore-specific config (used when mode: "agentcore")
  agentcore:
    # ARN of the deployed ACR agent runtime
    agent_runtime_arn: ""
    # S3 bucket for rollout data storage
    s3_bucket: ""
    # SQS queue URL for completion notifications
    sqs_url: ""
    # Experiment identifier used as S3 key prefix
    exp_id: "default"
    # Global timeout for collecting all episodes in a batch (seconds)
    timeout: 1800
    # SQS polling interval (seconds)
    poll_interval: 2.0
    # Max messages per SQS receive call
    batch_size: 10

# Episode Logging Configuration
episode_logging:
  log_episodes: false
  episode_log_dir: logs/${rllm.trainer.project_name}/${rllm.trainer.experiment_name}