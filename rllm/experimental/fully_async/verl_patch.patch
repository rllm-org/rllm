diff --git a/verl/trainer/ppo/core_algos.py b/verl/trainer/ppo/core_algos.py
index 2039fe56..f41c8856 100644
--- a/verl/trainer/ppo/core_algos.py
+++ b/verl/trainer/ppo/core_algos.py
@@ -1054,20 +1054,20 @@ def agg_loss(
     """
     if loss_agg_mode == "token-mean":
         if batch_num_tokens is None:
-            batch_num_tokens = loss_mask.sum()
+            batch_num_tokens = torch.clamp(loss_mask.sum(), min=1)
         loss = verl_F.masked_sum(loss_mat, loss_mask) / batch_num_tokens * dp_size
     elif loss_agg_mode == "seq-mean-token-sum":
         seq_losses = torch.sum(loss_mat * loss_mask, dim=-1)  # token-sum
         seq_mask = (torch.sum(loss_mask, dim=-1) > 0).float()  # exclude fully masked sequences
         if global_batch_size is None:
-            global_batch_size = seq_mask.sum()
+            global_batch_size = torch.clamp(seq_mask.sum(), min=1)
         loss = verl_F.masked_sum(seq_losses, seq_mask) / global_batch_size * dp_size  # seq-mean
     elif loss_agg_mode == "seq-mean-token-mean":
         seq_mask = torch.sum(loss_mask, dim=-1)  # per-sequence token count
         seq_losses = torch.sum(loss_mat * loss_mask, dim=-1) / (seq_mask + 1e-8)  # token-mean
         seq_mask = (seq_mask > 0).float()  # exclude fully masked sequences
         if global_batch_size is None:
-            global_batch_size = seq_mask.sum()
+            global_batch_size = torch.clamp(seq_mask.sum(), min=1)
         loss = verl_F.masked_sum(seq_losses, seq_mask) / global_batch_size * dp_size  # seq-mean
     elif loss_agg_mode == "seq-mean-token-sum-norm":
         seq_losses = torch.sum(loss_mat * loss_mask, dim=-1)
@@ -1856,8 +1856,8 @@ def kl_penalty(logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor, kl_pe
 
     """
     The expectation of k1 and k3 estimator is the expectaed value of KL, but the expected gradient of k1 and k3
-    estimator is not the expectaed gradient of KL. On the other hand k2 estimator gives right gradient estimator, 
-    so we use a straight through trick here if the kl_penalty method ends with '+', .e.g., k3+. 
+    estimator is not the expectaed gradient of KL. On the other hand k2 estimator gives right gradient estimator,
+    so we use a straight through trick here if the kl_penalty method ends with '+', .e.g., k3+.
     """
     backward_score = 0.5 * (logprob - ref_logprob).square()
 
diff --git a/verl/workers/actor/dp_actor.py b/verl/workers/actor/dp_actor.py
index d524f0e2..3314e755 100644
--- a/verl/workers/actor/dp_actor.py
+++ b/verl/workers/actor/dp_actor.py
@@ -538,7 +538,8 @@ class DataParallelPPOActor(BasePPOActor):
 
         # Split to make minibatch iterator for updating the actor
         # See PPO paper for details. https://arxiv.org/abs/1707.06347
-        mini_batches = data.split(self.config.ppo_mini_batch_size)
+        mini_batches = data.split(data.batch.batch_size[0])
+        assert len(mini_batches) == 1
 
         on_policy = len(mini_batches) == 1 and self.config.ppo_epochs == 1
 
@@ -551,6 +552,7 @@ class DataParallelPPOActor(BasePPOActor):
                 if self.config.use_dynamic_bsz:
                     max_token_len = self.config.ppo_max_token_len_per_gpu * self.ulysses_sequence_parallel_size
                     micro_batches, _ = prepare_dynamic_batch(mini_batch, max_token_len=max_token_len)
+                    mini_batch_num_tokens = mini_batch.batch["response_mask"].sum().item()
                 else:
                     self.gradient_accumulation = (
                         self.config.ppo_mini_batch_size // self.config.ppo_micro_batch_size_per_gpu
@@ -573,7 +575,8 @@ class DataParallelPPOActor(BasePPOActor):
                     calculate_entropy = self.config.calculate_entropy or (entropy_coeff != 0)
 
                     if self.config.use_dynamic_bsz:
-                        loss_scale_factor = response_mask.shape[0] / self.config.ppo_mini_batch_size
+                        micro_batch_num_tokens = micro_batch.batch["response_mask"].sum().item()
+                        loss_scale_factor = micro_batch_num_tokens / mini_batch_num_tokens
                     else:
                         loss_scale_factor = 1 / self.gradient_accumulation
 
@@ -618,7 +621,7 @@ class DataParallelPPOActor(BasePPOActor):
 
                     # Skip if using bypass_mode loss (metrics already computed in pg_metrics)
                     rollout_log_prob = model_inputs.get("rollout_log_probs", None)
-                    if loss_mode != "bypass_mode" and rollout_log_prob is not None:
+                    if loss_mode != "bypass_mode" and rollout_log_prob is not None and response_mask.any():
                         # Compute metrics using CURRENT policy π_θ vs π_rollout
                         # Tracks evolving off-policy gap as π_θ updates during mini-batch training
                         from verl.trainer.ppo.rollout_corr_helper import compute_rollout_corr_metrics_from_logprobs
diff --git a/verl/workers/actor/megatron_actor.py b/verl/workers/actor/megatron_actor.py
index 7fdaa6e9..1a9dd75a 100644
--- a/verl/workers/actor/megatron_actor.py
+++ b/verl/workers/actor/megatron_actor.py
@@ -382,8 +382,13 @@ class MegatronPPOActor(BasePPOActor):
         else:
             data = data.select(batch_keys=select_keys)
 
+        # Use actual batch size as mini_batch_size to ensure exactly one mini-batch.
+        # This avoids the batch_size % mini_batch_size != 0 assertion when the batch
+        # size is variable (e.g. fully async training). Micro-batching within each
+        # mini-batch is handled by forward_backward_batch (supports use_dynamic_bsz).
+        mini_batch_size= data.batch.batch_size[0]
         return data.make_iterator(
-            mini_batch_size=self.config.ppo_mini_batch_size,
+            mini_batch_size=mini_batch_size,
             epochs=self.config.ppo_epochs,
             seed=self.config.data_loader_seed,
             dataloader_kwargs={"shuffle": self.config.shuffle},
@@ -521,7 +526,7 @@ class MegatronPPOActor(BasePPOActor):
 
                 # Skip if using bypass_mode loss (metrics already computed in pg_metrics)
                 rollout_log_prob = data.get("rollout_log_probs", None)
-                if loss_mode != "bypass_mode" and rollout_log_prob is not None:
+                if loss_mode != "bypass_mode" and rollout_log_prob is not None and response_mask.any():
                     # Compute metrics using CURRENT policy π_θ vs π_rollout
                     # Tracks evolving off-policy gap as π_θ updates during mini-batch training
                     from verl.trainer.ppo.rollout_corr_helper import compute_rollout_corr_metrics_from_logprobs
diff --git a/verl/workers/megatron_workers.py b/verl/workers/megatron_workers.py
index 1323afbc..90ccc1f2 100644
--- a/verl/workers/megatron_workers.py
+++ b/verl/workers/megatron_workers.py
@@ -522,6 +522,7 @@ class ActorRolloutRefWorker(MegatronWorker, DistProfilerExtension):
         rollout_device_mesh = init_device_mesh(
             get_device_name(), mesh_shape=(dp, infer_tp, infer_pp), mesh_dim_names=["dp", "infer_tp", "infer_pp"]
         )
+        self.rollout_device_mesh = rollout_device_mesh
 
         is_collect = (
             rollout_device_mesh["infer_tp"].get_local_rank() == 0
