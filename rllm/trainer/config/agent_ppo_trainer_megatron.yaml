# Megatron configuration for agent PPO training
# This config extends agent_ppo_trainer with Megatron-specific settings

hydra:
  searchpath:
    - pkg://verl.trainer.config

defaults:
  - ppo_megatron_trainer  # Load base ppo_megatron_trainer which has proper megatron configs
  - _self_

# Override with agent-specific settings
actor_rollout_ref:
  # Override the default model path from ppo_megatron_trainer
  model:
    path: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B  # Use this model instead of ~/models/deepseek-llm-7b-chat
    enable_gradient_checkpointing: True  # Save GPU memory by recomputing activations

  # Set micro batch sizes for megatron
  actor:
    ppo_micro_batch_size_per_gpu: 1  # Required for megatron
    ppo_mini_batch_size: 8  # Small batch for testing, must be <= train_batch_size
    megatron:
      param_offload: True  # Offload parameters to CPU to save GPU memory
      grad_offload: False  # Keep gradients on GPU for speed
      optimizer_offload: False  # Keep optimizer on GPU

  rollout:
    mode: async # only async is supported
    log_prob_micro_batch_size_per_gpu: 1  # Required for rollout
    max_model_len: 4096  # Reduce max sequence length to save memory (default is often 8192+)
    gpu_memory_utilization: 0.8  # Use only 80% of GPU memory for safety
    agent:
      num_workers: 0 # we bypass the agent loop
    val_kwargs:
      do_sample: True # if do_sample is False, temperature is overridden to 0.0

  ref:
    log_prob_micro_batch_size_per_gpu: 1  # Required for reference model
    megatron:
      param_offload: True  # Offload reference model to CPU (not used during training)

# Critic configuration
critic:
  ppo_micro_batch_size_per_gpu: 1  # Required for megatron critic
  model:
    path: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B  # Use same model as actor
    tokenizer_path: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B  # Tokenizer path
    enable_gradient_checkpointing: True  # Save memory for critic
  megatron:
    param_offload: False  # Keep critic on GPU for training
    optimizer_offload: True  # Offload optimizer states to save memory

# Data configuration - reduce batch sizes
data:
  gen_batch_size: ${mul:${data.train_batch_size},${rllm.rejection_sample.multiplier}}
  train_batch_size: 32  # Small batch for testing, must be >= ppo_mini_batch_size (8)
  max_prompt_length: 1024  # Reduce max prompt length
  max_response_length: 2048  # Reduce max response length

# Include rLLM agent configuration

rllm:
  agent:
    name: miniwobagent
    max_steps: 20
    trajectory_timeout: null
    overlong_filter: False
    agent_args: {}
    engine_args: {}
  env:
    name: custom
    env_args: {}
  workflow:
    use_workflow: False
    name: single_turn_workflow
    workflow_args:
      max_prompt_length: ${data.max_prompt_length}
      max_response_length: ${data.max_response_length}
      timeout: 1e6
      gamma: 0.0 # no discounting
      reward_bonus_coeff: 0.0 # no reward shaping
      accumulate_response_length: null
    n_parallel_tasks: 256
    retry_limit: 3
  disable_thinking: False
  mask_truncated_samples: False
  stepwise_advantage:
    enable: False
    mode: broadcast # [broadcast, per_step]
    normalize_by_steps: False
  compact_filtering:
    enable: False
    mask_max_prompt_length_exceeded: True
    mask_max_response_length_exceeded: True
    mask_max_turns_exceeded: True
    mask_timeout: True
  rejection_sample:
    enable: False
    multiplier: 1