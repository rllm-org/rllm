# Megatron configuration for agent PPO training
# This config extends agent_ppo_trainer with Megatron-specific settings

hydra:
  searchpath:
    - pkg://verl.trainer.config

defaults:
  - ppo_megatron_trainer  # Load base ppo_megatron_trainer which has proper megatron configs
  - _self_

# Override trainer settings to disable wandb
trainer:
  logger: ['console']  # Only console logging, no wandb

# Override with agent-specific settings
actor_rollout_ref:
  # Override the default model path from ppo_megatron_trainer
  model:
    path: Qwen/Qwen2.5-0.5B-Instruct  # Smallest supported model (500M params)
    enable_gradient_checkpointing: True  # Save GPU memory by recomputing activations

  # Set micro batch sizes for megatron
  actor:
    ppo_micro_batch_size_per_gpu: 1  
    ppo_mini_batch_size: 1  # With DP=1, minimum is 1
    megatron:
      param_offload: True  # Offload parameters to CPU to save GPU memory
      grad_offload: True  # Also offload gradients to save more memory
      optimizer_offload: True  # Offload optimizer to save significant memory

  rollout:
    mode: async # only async is supported
    log_prob_micro_batch_size_per_gpu: 1  # Required for rollout
    max_model_len: 1024  # Total context for prompts + responses
    max_num_batched_tokens: 1024  # Match max_model_len
    gpu_memory_utilization: 0.1  # 10% of GPU memory for vLLM (~8 GiB per GPU)
    enforce_eager: True  # Disable CUDA graphs completely
    max_num_seqs: 1  # Process only 1 sequence at a time
    # tensor_model_parallel_size set via bash script (TP=2)
    agent:
      num_workers: 0 # we bypass the agent loop
    val_kwargs:
      do_sample: True # if do_sample is False, temperature is overridden to 0.0

  ref:
    log_prob_micro_batch_size_per_gpu: 1  # Required for reference model
    megatron:
      param_offload: True  # Offload reference model to CPU (not used during training)

# Critic configuration
critic:
  ppo_micro_batch_size_per_gpu: 1  # Absolute minimum per GPU
  ppo_mini_batch_size: 1  # With DP=1, minimum is 1
  model:
    path: Qwen/Qwen2.5-0.5B-Instruct  # Smallest supported model (500M params)
    tokenizer_path: Qwen/Qwen2.5-0.5B-Instruct  # Qwen tokenizer
    enable_gradient_checkpointing: True  # Save memory for critic
  megatron:
    param_offload: True  # Also offload critic params to save memory
    optimizer_offload: True  # Offload optimizer states to save memory

# Data configuration - reduce batch sizes
data:
  gen_batch_size: ${mul:${data.train_batch_size},${rllm.rejection_sample.multiplier}}
  train_batch_size: 1  # With 2 GPUs and TP=2, DP=1 so minimum is 1
  max_prompt_length: 512  # Increased to handle actual prompts
  max_response_length: 512  # Increased for reasonable responses

# Include rLLM agent configuration

rllm:
  agent:
    name: testagent
    max_steps: 20
    trajectory_timeout: nullch
    overlong_filter: False
    agent_args: {}
    engine_args: {}
  env:
    name: custom
    env_args: {}
  workflow:
    use_workflow: False
    name: single_turn_workflow
    workflow_args:
      max_prompt_length: ${data.max_prompt_length}
      max_response_length: ${data.max_response_length}
      timeout: 1e6
      gamma: 0.0 # no discounting
      reward_bonus_coeff: 0.0 # no reward shaping
      accumulate_response_length: null
    n_parallel_tasks: 256
    retry_limit: 3
  disable_thinking: False
  mask_truncated_samples: False
  stepwise_advantage:
    enable: False
    mode: broadcast # [broadcast, per_step]
    normalize_by_steps: False
  compact_filtering:
    enable: False
    mask_max_prompt_length_exceeded: True
    mask_max_response_length_exceeded: True
    mask_max_turns_exceeded: True
    mask_timeout: True
  rejection_sample:
    enable: False
    multiplier: 1